# train.py
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Subset
from torch.amp import autocast, GradScaler
import yaml
import os
import numpy as np
from torch.utils.tensorboard import SummaryWriter
from models.CA_unet import ImprovedConditionalUNet
from datasets.celeba_dataset import CelebADataset
from utils.flow_utils import flow_matching_loss
from utils.visualize import save_inpainting_result

def main():
    # åŠ è½½é…ç½®
    with open("config/config.yaml", 'r') as f:
        config = yaml.safe_load(f)
    
    # è®¾ç½®éšæœºç§å­
    seed = config.get('seed', 42)
    if seed is not None:
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)
        np.random.seed(seed)
    
    # æ›¿æ¢é…ç½®ä¸­çš„å˜é‡å ä½ç¬¦
    for key in config:
        if isinstance(config[key], str) and '${seed}' in config[key]:
            config[key] = config[key].replace('${seed}', str(seed))
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    os.makedirs(os.path.dirname(config['model_save_path']), exist_ok=True)
    os.makedirs(config['results_dir'], exist_ok=True)
    
    # åˆ›å»ºTensorBoardæ—¥å¿—ç›®å½•
    writer = SummaryWriter(log_dir=config['tensorboard_log_dir'])
    
    # è®°å½•é…ç½®ä¿¡æ¯
    writer.add_text('Config', str(config))

    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    print("Loading dataset...")
    try:
        full_dataset = CelebADataset(config['data_root'], img_size=config['img_size'])
        # é™åˆ¶æ•°æ®é›†å¤§å°
        max_dataset_size = config.get('max_dataset_size', 8000)
        if len(full_dataset) > max_dataset_size:
            dataset = Subset(full_dataset, range(min(max_dataset_size, len(full_dataset))))
            print(f"Dataset limited to {len(dataset)} samples")
        else:
            dataset = full_dataset
            print(f"Dataset loaded with {len(dataset)} samples")
    except Exception as e:
        print(f"Failed to load dataset: {e}")
        return
    
    dataloader = DataLoader(
        dataset, 
        batch_size = 64, 
        shuffle=True, 
        num_workers=config.get('num_workers', 4),
        pin_memory=True,
        persistent_workers=True if config.get('num_workers', 4) > 0 else False
    )

    # åˆå§‹åŒ–æ¨¡å‹
    print("Initializing model...")
    model = ImprovedConditionalUNet(
        in_channels=config.get('in_channels', 3),
        width=32
    )
    model = model.to(device)
    print(f"Model initialized with {sum(p.numel() for p in model.parameters())} parameters")
    
    # è®¾ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
    optimizer = torch.optim.AdamW(
        model.parameters(), 
        lr=config['learning_rate'],
        weight_decay=config.get('weight_decay', 1e-4)
    )
    
    scheduler_type = config.get('scheduler', 'cosine')
    if scheduler_type == 'cosine':
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, 
            T_max=config['num_epochs']
        )
    elif scheduler_type == 'step':
        scheduler = torch.optim.lr_scheduler.StepLR(
            optimizer,
            step_size=config.get('step_size', 20),
            gamma=config.get('gamma', 0.5)
        )
    else:
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, 
            T_max=config['num_epochs']
        )
    
    # åˆ›å»ºæ¢¯åº¦ç¼©æ”¾å™¨ç”¨äºæ··åˆç²¾åº¦è®­ç»ƒ
    scaler = torch.amp.GradScaler('cuda') if torch.cuda.is_available() else None

    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ ({config['num_epochs']} epochs)...")
    print(f"   Batch size: {config['batch_size']}")
    print(f"   Learning rate: {config['learning_rate']}")
    print(f"   Dataset size: {len(dataset)}")
    
    # ç”¨äºè®°å½•è®­ç»ƒæŸå¤±
    train_losses = []
    
    # å›ºå®šä¸€æ‰¹éªŒè¯æ ·æœ¬ç”¨äºå¯è§†åŒ–
    val_sample = None
    val_mask = None

    # è®­ç»ƒå¾ªç¯
    for epoch in range(config['num_epochs']):
        model.train()
        total_loss = 0.0
        num_batches = 0
        
        # éå†æ‰€æœ‰batch
        for i, (masked, mask, clean) in enumerate(dataloader):
            # ç¡®ä¿æ‰€æœ‰å¼ é‡éƒ½åœ¨ç›¸åŒè®¾å¤‡ä¸Š
            masked = masked.to(device, non_blocking=True)
            clean = clean.to(device, non_blocking=True)
            mask = mask.to(device, non_blocking=True)

            # åœ¨ç¬¬ä¸€ä¸ªepochä¿å­˜éªŒè¯æ ·æœ¬
            if epoch == 0 and val_sample is None:
                val_sample = (masked[:4].clone(), clean[:4].clone())
                val_mask = mask[:4].clone()
            
            # æ£€æŸ¥è¾“å…¥æ˜¯å¦æœ‰æ•ˆ
            if torch.isnan(masked).any() or torch.isnan(clean).any():
                print(f"NaN values detected in batch {i}, skipping...")
                continue
                
            try:
                # ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦
                with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):
                    loss = flow_matching_loss(model, clean, masked)
                
                # æ£€æŸ¥æŸå¤±æ˜¯å¦æœ‰æ•ˆ
                if torch.isnan(loss) or torch.isinf(loss):
                    print(f"Invalid loss value in batch {i}, skipping...")
                    continue
                    
                # åå‘ä¼ æ’­ä½¿ç”¨ç¼©æ”¾å™¨ï¼ˆå¦‚æœä½¿ç”¨CUDAï¼‰
                optimizer.zero_grad()
                if scaler is not None:
                    scaler.scale(loss).backward()
                    # æ¢¯åº¦è£å‰ª
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                    # æ›´æ–°æƒé‡
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    loss.backward()
                    # æ¢¯åº¦è£å‰ª
                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                    optimizer.step()
                
                total_loss += loss.item()
                num_batches += 1
                
                # è®°å½•æ¯ä¸ªbatchçš„æŸå¤±
                writer.add_scalar('Batch/Loss', loss.item(), epoch * len(dataloader) + i)
                
                # æ‰“å°è¿›åº¦
                if (i + 1) % 10 == 0:
                    print(f"  Epoch [{epoch+1}/{config['num_epochs']}], Batch [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}")
                    
            except RuntimeError as e:
                print(f"Error in batch {i}: {e}")
                print("Skipping this batch...")
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
                continue

        # è®¡ç®—å¹³å‡æŸå¤±
        if num_batches > 0:
            avg_loss = total_loss / num_batches
            train_losses.append(avg_loss)
            
            # è®°å½•å­¦ä¹ ç‡
            current_lr = optimizer.param_groups[0]['lr']
            
            # è®°å½•epochçº§åˆ«çš„æŸå¤±å’Œå­¦ä¹ ç‡
            writer.add_scalar('Epoch/Loss', avg_loss, epoch)
            writer.add_scalar('Epoch/Learning_Rate', current_lr, epoch)
            
            print(f"Epoch [{epoch+1}/{config['num_epochs']}] completed. Avg Loss: {avg_loss:.4f}, LR: {current_lr:.6f}")
        else:
            print(f"Epoch [{epoch+1}/{config['num_epochs']}] completed with no valid batches.")
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()

        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹å’Œå¯è§†åŒ–ç»“æœ
        if (epoch + 1) % config.get('checkpoint_interval', 10) == 0:
            # ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹
            checkpoint_path = f"checkpoints/model_epoch_{epoch+1}.pth"
            os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'loss': avg_loss if num_batches > 0 else None,
                'config': config
            }, checkpoint_path)
            print(f"Checkpoint saved to {checkpoint_path}")
            
            # ç”Ÿæˆå¯è§†åŒ–ç»“æœ
            model.eval()
            with torch.no_grad():
                try:
                    # ä¿å­˜å½“å‰batchçš„å¯è§†åŒ–ç»“æœ
                    save_inpainting_result(
                        model,
                        (masked[:4], clean[:4]),
                        mask[:4],
                        device,
                        f"{config['results_dir']}/epoch_{epoch+1}.png"
                    )
                    
                    # ä¿å­˜å›ºå®šæ ·æœ¬çš„å¯è§†åŒ–ç»“æœï¼Œä¾¿äºæ¯”è¾ƒè®­ç»ƒè¿‡ç¨‹
                    if val_sample is not None and val_mask is not None:
                        val_masked, val_clean = val_sample
                        val_masked, val_clean = val_masked.to(device), val_clean.to(device)
                        save_inpainting_result(
                            model,
                            (val_masked, val_clean),
                            val_mask,
                            device,
                            f"{config['results_dir']}/fixed_sample_epoch_{epoch+1}.png"
                        )
                    
                except Exception as e:
                    print(f"Failed to save visualization: {e}")
            model.train()

        # æ˜¾å­˜æ¸…ç†
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    try:
        final_model_path = config.get('model_save_path', "checkpoints/final_model.pth")
        os.makedirs(os.path.dirname(final_model_path), exist_ok=True)
        torch.save(model.state_dict(), final_model_path)
        print(f"âœ… è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆæ¨¡å‹å·²ä¿å­˜è‡³ {final_model_path}")
    except Exception as e:
        print(f"Failed to save final model: {e}")

    writer.close()
    print("Training completed and TensorBoard logs saved.")

if __name__ == "__main__":
    main()