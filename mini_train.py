import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Subset
from torch.amp import autocast, GradScaler
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
import torch.distributed as dist
import torch.multiprocessing as mp
from models.CA_unet import ImprovedConditionalUNet
from datasets.celeba_dataset import CelebADataset
from utils.flow_utils import flow_matching_loss
from utils.visualize import save_inpainting_result
import yaml
import os
import matplotlib.pyplot as plt
import numpy as np
from torch.utils.tensorboard import SummaryWriter

def setup_devices():
    """è®¾ç½®å¤šGPUç¯å¢ƒ"""
    if torch.cuda.is_available() and torch.cuda.device_count() >= 2:
        # ä½¿ç”¨å¤šä¸ªGPU
        device_ids = list(range(torch.cuda.device_count()))  # ä½¿ç”¨æ‰€æœ‰å¯ç”¨GPU
        device = torch.device("cuda:0")  # ä¸»è®¾å¤‡
        print(f"Using devices: {[f'cuda:{i}' for i in device_ids]}")
        return device, device_ids
    elif torch.cuda.is_available():
        device = torch.device("cuda:0")
        device_ids = [0]
        print("Using single GPU: cuda:0")
        return device, device_ids
    else:
        device = torch.device("cpu")
        print("Using CPU")
        return device, None

def setup_ddp(rank, world_size):
    """è®¾ç½®DDPç¯å¢ƒ"""
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    dist.init_process_group("nccl", rank=rank, world_size=world_size)

def cleanup_ddp():
    """æ¸…ç†DDPç¯å¢ƒ"""
    dist.destroy_process_group()

def main_worker(gpu, ngpus_per_node, config):
    """DDPå·¥ä½œè¿›ç¨‹ä¸»å‡½æ•°"""
    # è®¾ç½®DDP
    setup_ddp(gpu, ngpus_per_node)
    
    # è®¾ç½®è®¾å¤‡
    torch.cuda.set_device(gpu)
    device = torch.device(f"cuda:{gpu}")
    
    # è®¾ç½®éšæœºç§å­
    seed = config.get('seed', 42)
    if seed is not None:
        torch.manual_seed(seed)
        torch.cuda.manual_seed(seed)
        np.random.seed(seed)
        torch.cuda.manual_seed_all(seed)
    
    # æ›¿æ¢é…ç½®ä¸­çš„å˜é‡å ä½ç¬¦
    for key in config:
        if isinstance(config[key], str) and '${seed}' in config[key]:
            config[key] = config[key].replace('${seed}', str(seed))
    
    # åªåœ¨ä¸»è¿›ç¨‹ä¸­åˆ›å»ºç›®å½•å’ŒTensorBoard
    if gpu == 0:
        os.makedirs(os.path.dirname(config['model_save_path']), exist_ok=True)
        os.makedirs(config['results_dir'], exist_ok=True)
        writer = SummaryWriter(log_dir=config['tensorboard_log_dir'])
        writer.add_text('Config', str(config))
    else:
        writer = None

    # åˆ›å»ºæ•°æ®é›†
    print(f"GPU {gpu}: Loading dataset...")
    try:
        full_dataset = CelebADataset(config['data_root'], img_size=config['img_size'])
        dataset = Subset(full_dataset, range(min(8000, len(full_dataset))))
        if gpu == 0:
            print(f"Dataset loaded with {len(dataset)} samples ")
    except Exception as e:
        print(f"GPU {gpu}: Failed to load dataset: {e}")
        return
    
    # ä½¿ç”¨DistributedSampler
    sampler = DistributedSampler(dataset, num_replicas=ngpus_per_node, rank=gpu)
    
    # æ ¹æ®GPUæ•°é‡è°ƒæ•´batch size
    base_batch_size = min(config['batch_size'], 24)
    effective_batch_size = base_batch_size * ngpus_per_node
    
    dataloader = DataLoader(
        dataset, 
        batch_size=base_batch_size,  # æ¯ä¸ªGPUçš„batch size
        shuffle=False,  # DistributedSamplerä¼šå¤„ç†shuffle
        sampler=sampler,
        num_workers=4,
        pin_memory=True,
        persistent_workers=True
    )

    # åˆå§‹åŒ–æ¨¡å‹
    print(f"GPU {gpu}: Initializing model...")
    model = ImprovedConditionalUNet(in_channels=3, width=config.get('model_width', 64)) 
    model = model.to(device)
    
    # ä½¿ç”¨DistributedDataParallel
    model = DDP(model, device_ids=[gpu])
    print(f"GPU {gpu}: Model parallelized with DDP")
    
    # è®¾ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
    optimizer = torch.optim.AdamW(
        model.parameters(), 
        lr=config['learning_rate'],
        weight_decay=config.get('weight_decay', 1e-4)
    )
    
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, 
        T_max=config['num_epochs']
    )
    
    # åˆ›å»ºæ¢¯åº¦ç¼©æ”¾å™¨ç”¨äºæ··åˆç²¾åº¦è®­ç»ƒ
    scaler = GradScaler('cuda')

    if gpu == 0:
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒ ({config['num_epochs']} epochs)...")
    
    # ç”¨äºè®°å½•è®­ç»ƒæŸå¤±
    train_losses = []
    
    # å›ºå®šä¸€æ‰¹éªŒè¯æ ·æœ¬ç”¨äºå¯è§†åŒ–ï¼ˆåªåœ¨ä¸»GPUä¸Šï¼‰
    val_sample = None
    val_mask = None

    # è®­ç»ƒå¾ªç¯
    for epoch in range(config['num_epochs']):
        sampler.set_epoch(epoch)  # é‡è¦ï¼šç¡®ä¿æ¯ä¸ªepochæ•°æ®shuffle
        
        model.train()
        total_loss = 0.0
        num_batches = 0
        
        # éå†æ‰€æœ‰batch
        for i, (masked, mask, clean) in enumerate(dataloader):
            # ç¡®ä¿æ‰€æœ‰å¼ é‡éƒ½åœ¨ç›¸åŒè®¾å¤‡ä¸Š
            masked, clean, mask = masked.to(device), clean.to(device), mask.to(device)

            # åœ¨ç¬¬ä¸€ä¸ªepochä¿å­˜éªŒè¯æ ·æœ¬ï¼ˆåªåœ¨ä¸»GPUä¸Šï¼‰
            if gpu == 0 and epoch == 0 and val_sample is None:
                val_sample = (masked[:4].clone(), clean[:4].clone())
                val_mask = mask[:4].clone()
            
            # æ£€æŸ¥è¾“å…¥æ˜¯å¦æœ‰æ•ˆ
            if torch.isnan(masked).any() or torch.isnan(clean).any():
                print(f"GPU {gpu}: NaN values detected in batch {i}, skipping...")
                continue
                
            try:
                # ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦
                with autocast(device_type='cuda'):
                    loss = flow_matching_loss(model, clean, masked)
                
                # æ£€æŸ¥æŸå¤±æ˜¯å¦æœ‰æ•ˆ
                if torch.isnan(loss) or torch.isinf(loss):
                    print(f"GPU {gpu}: Invalid loss value in batch {i}, skipping...")
                    continue
                    
                # åå‘ä¼ æ’­ä½¿ç”¨ç¼©æ”¾å™¨
                optimizer.zero_grad()
                scaler.scale(loss).backward()
                
                # æ¢¯åº¦è£å‰ª
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                
                # æ›´æ–°æƒé‡
                scaler.step(optimizer)
                scaler.update()
                
                total_loss += loss.item()
                num_batches += 1
                
                # è®°å½•æ¯ä¸ªbatchçš„æŸå¤±ï¼ˆåªåœ¨ä¸»GPUä¸Šï¼‰
                if gpu == 0 and writer is not None:
                    writer.add_scalar('Batch/Loss', loss.item(), epoch * len(dataloader) + i)
                
                # æ‰“å°è¿›åº¦ï¼ˆæ¯10ä¸ªbatchæ‰“å°ä¸€æ¬¡ä»¥èŠ‚çœè¾“å‡ºï¼Œåªåœ¨ä¸»GPUä¸Šï¼‰
                if gpu == 0 and (i + 1) % 10 == 0:
                    print(f"  Epoch [{epoch+1}/{config['num_epochs']}], Batch [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}")
                    
            except RuntimeError as e:
                print(f"GPU {gpu}: Error in batch {i}: {e}")
                print("Skipping this batch...")
                torch.cuda.empty_cache()
                continue

        # è®¡ç®—å¹³å‡æŸå¤±
        if num_batches > 0:
            avg_loss = total_loss / num_batches
            train_losses.append(avg_loss)
            
            # è®°å½•epochçº§åˆ«æŸå¤±å’Œå­¦ä¹ ç‡ï¼ˆåªåœ¨ä¸»GPUä¸Šï¼‰
            if gpu == 0 and writer is not None:
                writer.add_scalar('Epoch/Loss', avg_loss, epoch)
                current_lr = optimizer.param_groups[0]['lr']
                writer.add_scalar('Epoch/Learning_Rate', current_lr, epoch)
            
            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step()
            
            if gpu == 0:
                print(f"Epoch [{epoch+1}/{config['num_epochs']}] completed. Avg Loss: {avg_loss:.4f}")

        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹å’Œå¯è§†åŒ–ç»“æœï¼ˆåªåœ¨ä¸»GPUä¸Šï¼‰
        if gpu == 0 and (epoch + 1) % 5 == 0:
            # ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹
            checkpoint_path = f"checkpoints/model_epoch_{epoch+1}.pth"
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.module.state_dict(),  # ä½¿ç”¨moduleè·å–åŸå§‹æ¨¡å‹
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': avg_loss,
                'config': config
            }, checkpoint_path)
            print(f"Checkpoint saved to {checkpoint_path}")
            
            # ç”Ÿæˆå¯è§†åŒ–ç»“æœ
            model.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
            with torch.no_grad():
                try:
                    # ä¿å­˜å½“å‰batchçš„å¯è§†åŒ–ç»“æœ
                    save_inpainting_result(
                        model.module,  # ä½¿ç”¨moduleè·å–åŸå§‹æ¨¡å‹
                        (masked[:4], clean[:4]),
                        mask[:4],
                        device,
                        f"{config['results_dir']}/epoch_{epoch+1}.png"
                    )
                    
                    # ä¿å­˜å›ºå®šæ ·æœ¬çš„å¯è§†åŒ–ç»“æœï¼Œä¾¿äºæ¯”è¾ƒè®­ç»ƒè¿‡ç¨‹
                    if val_sample is not None and val_mask is not None:
                        val_masked, val_clean = val_sample
                        val_masked, val_clean = val_masked.to(device), val_clean.to(device)
                        save_inpainting_result(
                            model.module,  # ä½¿ç”¨moduleè·å–åŸå§‹æ¨¡å‹
                            (val_masked, val_clean),
                            val_mask,
                            device,
                            f"{config['results_dir']}/fixed_sample_epoch_{epoch+1}.png"
                        )
                    
                except Exception as e:
                    print(f"Failed to save visualization: {e}")
            model.train()  # æ¢å¤è®­ç»ƒæ¨¡å¼

        # æ˜¾å­˜æ¸…ç†
        torch.cuda.empty_cache()

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ˆåªåœ¨ä¸»GPUä¸Šï¼‰
    if gpu == 0:
        try:
            final_model_path = config.get('model_save_path', "checkpoints/final_model.pth")
            torch.save(model.module.state_dict(), final_model_path)  # ä½¿ç”¨moduleè·å–åŸå§‹æ¨¡å‹
            print(f"âœ… è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆæ¨¡å‹å·²ä¿å­˜è‡³ {final_model_path}")
        except Exception as e:
            print(f"Failed to save final model: {e}")

        if writer is not None:
            writer.close()
        print("Training completed and TensorBoard logs saved.")

def main():
    # åŠ è½½é…ç½®
    with open("config/config.yaml", 'r') as f:
        config = yaml.safe_load(f)
    
    # è·å–GPUæ•°é‡
    ngpus_per_node = torch.cuda.device_count()
    print(f"Available GPUs: {ngpus_per_node}")
    
    if ngpus_per_node > 1:
        # ä½¿ç”¨å¤šGPU DDPè®­ç»ƒ
        mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, config))
    else:
        # å•GPUè®­ç»ƒ
        main_worker(0, 1, config)

if __name__ == "__main__":
    main()